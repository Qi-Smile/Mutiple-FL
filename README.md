# Multi-Server Federated Learning

ä¸€ä¸ªåŸºäº PyTorch å’Œ Flower çš„å¤šæœåŠ¡å™¨è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒçœŸæ­£çš„å¤šGPUå¹¶è¡Œè®­ç»ƒã€‚

## âœ¨ ç‰¹æ€§

- âœ… **å¤šæœåŠ¡å™¨æ¶æ„**ï¼šæ”¯æŒå¤šä¸ªå‚æ•°æœåŠ¡å™¨å¹¶è¡Œèšåˆ
- âœ… **Floweré›†æˆ**ï¼šé›†æˆFloweræ¡†æ¶ï¼Œæ”¯æŒ10+ç§èšåˆç­–ç•¥  
- âœ… **çœŸæ­£çš„å¤šGPUå¹¶è¡Œ**ï¼šä½¿ç”¨å¤šè¿›ç¨‹å®ç°çœŸæ­£çš„å¤šGPUå¹¶è¡Œè®­ç»ƒ
- âœ… **Non-IIDæ•°æ®**ï¼šæ”¯æŒDirichletåˆ†å¸ƒçš„æ•°æ®å¼‚æ„æ€§æ¨¡æ‹Ÿ
- âœ… **SwanLabé›†æˆ**ï¼šå®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–

## ğŸ–¥ï¸ GPUæ”¯æŒ

### å•GPUè®­ç»ƒï¼ˆæ¨èç”¨äºè°ƒè¯•ï¼‰
```bash
python scripts/run_flower_example.py \
  --num-clients 10 \
  --rounds 10 \
  --batch-size 128 \
  --max-workers 1
```

### å¤šGPUå¹¶è¡Œè®­ç»ƒï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰
```bash
python scripts/run_multigpu_simple.py \
  --num-clients 20 \
  --rounds 10 \
  --batch-size 128
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- å•GPU: 1xé€Ÿåº¦
- åŒGPU: ~2xé€Ÿåº¦  
- éœ€è¦: 2Ã— GPUæ˜¾å­˜

è¯¦è§ï¼š[docs/MULTI_GPU_GUIDE.md](docs/MULTI_GPU_GUIDE.md)

## ğŸ“¦ å®‰è£…

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd Mutiple-FL

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. å®‰è£…é¡¹ç›®ï¼ˆå¯ç¼–è¾‘æ¨¡å¼ï¼‰
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸç”Ÿå¤šæœåŠ¡å™¨FL

```bash
python scripts/run_example.py \
  --dataset cifar10 \
  --num-clients 10 \
  --num-servers 2 \
  --rounds 5
```

### 2. Floweré›†æˆç‰ˆæœ¬

```bash
python scripts/run_flower_example.py \
  --dataset mnist \
  --num-clients 10 \
  --num-servers 2 \
  --rounds 5 \
  --strategy fedavg
```

### 3. å¤šGPUå¹¶è¡Œè®­ç»ƒ

```bash
# è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰GPU
python scripts/run_multigpu_simple.py \
  --num-clients 20 \
  --rounds 10

# æŒ‡å®šç‰¹å®šGPU
python scripts/run_multigpu_simple.py \
  --gpu-ids 0 1 \
  --num-clients 20
```

## ğŸ“Š é¡¹ç›®ç»“æ„

```
Mutiple-FL/
â”œâ”€â”€ multi_server_fl/          # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ client.py             # åŸç”Ÿå®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ server.py             # åŸç”ŸæœåŠ¡å™¨å®ç°
â”‚   â”œâ”€â”€ coordinator.py        # åè°ƒå™¨
â”‚   â”œâ”€â”€ flower_client.py      # Flowerå®¢æˆ·ç«¯åŒ…è£…
â”‚   â”œâ”€â”€ flower_server.py      # FloweræœåŠ¡å™¨ï¼ˆå•GPUï¼‰
â”‚   â”œâ”€â”€ flower_server_multigpu.py  # â­ FloweræœåŠ¡å™¨ï¼ˆå¤šGPUï¼‰
â”‚   â”œâ”€â”€ data/                 # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ models/               # æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ utils.py              # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                  # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ run_example.py        # åŸç”Ÿå®ç°ç¤ºä¾‹
â”‚   â”œâ”€â”€ run_flower_example.py # Flowerå®ç°ç¤ºä¾‹
â”‚   â””â”€â”€ run_multigpu_simple.py # â­ å¤šGPUè®­ç»ƒç¤ºä¾‹
â”‚
â”œâ”€â”€ docs/                     # æ–‡æ¡£
â”‚   â”œâ”€â”€ FLOWER_GUIDE.md       # Flowerä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ MULTI_GPU_GUIDE.md    # â­ å¤šGPUå®Œæ•´æŒ‡å—
â”‚   â”œâ”€â”€ MULTI_GPU_SUMMARY.md  # å¤šGPUå¿«é€Ÿæ€»ç»“
â”‚   â””â”€â”€ WHY_GPU_CANNOT_PARALLEL.md  # GPUå¹¶è¡ŒåŸç†
â”‚
â”œâ”€â”€ data/                     # æ•°æ®é›†ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
â”œâ”€â”€ setup.py                  # åŒ…é…ç½®
â””â”€â”€ requirements.txt          # ä¾èµ–åˆ—è¡¨
```

## ğŸ“š æ–‡æ¡£

- [Floweré›†æˆæŒ‡å—](docs/FLOWER_GUIDE.md) - Floweræ¡†æ¶ä½¿ç”¨è¯´æ˜
- [å¤šGPUè®­ç»ƒæŒ‡å—](docs/MULTI_GPU_GUIDE.md) - å¤šGPUå¹¶è¡Œè®­ç»ƒå®Œæ•´æ•™ç¨‹
- [GPUå¹¶è¡ŒåŸç†](docs/WHY_GPU_CANNOT_PARALLEL.md) - ä¸ºä»€ä¹ˆå•GPUæ— æ³•å¹¶è¡Œ
- [å¤šGPUå¿«é€Ÿæ€»ç»“](docs/MULTI_GPU_SUMMARY.md) - å¤šGPUè®­ç»ƒå¿«é€Ÿä¸Šæ‰‹

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### å¤šGPUå¹¶è¡Œç­–ç•¥

**å•GPUé—®é¢˜**ï¼š
- âŒ ThreadPoolExecutorï¼šå‡å¹¶è¡Œï¼Œåè€Œæ›´æ…¢
- âŒ ProcessPoolExecutor + å•GPUï¼šä»ç„¶ä¸²è¡Œ

**å¤šGPUè§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… ProcessPoolExecutor + å¤šGPUï¼šçœŸæ­£å¹¶è¡Œ
- âœ… æ¯ä¸ªè¿›ç¨‹ç»‘å®šç‹¬ç«‹GPU
- âœ… é€Ÿåº¦æå‡æ¥è¿‘GPUæ•°é‡å€æ•°

è¯¦è§ï¼š[docs/WHY_GPU_CANNOT_PARALLEL.md](docs/WHY_GPU_CANNOT_PARALLEL.md)

## âš™ï¸ é…ç½®é€‰é¡¹

### é€šç”¨å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--dataset` | cifar10 | æ•°æ®é›† (mnist/cifar10) |
| `--num-clients` | 10 | å®¢æˆ·ç«¯æ•°é‡ |
| `--num-servers` | 2 | æœåŠ¡å™¨æ•°é‡ |
| `--rounds` | 5 | å…¨å±€è®­ç»ƒè½®æ¬¡ |
| `--local-epochs` | 2 | æœ¬åœ°è®­ç»ƒè½®æ¬¡ |
| `--batch-size` | 32 | æ‰¹æ¬¡å¤§å° |
| `--alpha` | 0.5 | Dirichletå‚æ•° |

### Flowerç‰¹æœ‰å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--strategy` | fedavg | èšåˆç­–ç•¥ (fedavg/fedprox/fedadam...) |
| `--max-workers` | None | å¹¶è¡Œworkeræ•°ï¼ˆä¸æ¨èåœ¨å•GPUä½¿ç”¨ï¼‰ |

### å¤šGPUç‰¹æœ‰å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--gpu-ids` | None | æŒ‡å®šGPU ID (å¦‚: --gpu-ids 0 1) |
| `--auto-gpu` | True | è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰GPU |

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å•GPUä¼˜åŒ–
```bash
# å¢å¤§batch sizeï¼ˆæœ€æœ‰æ•ˆï¼‰
--batch-size 128  # è€Œä¸æ˜¯32

# ä¸²è¡Œæ‰§è¡Œ
--max-workers 1  # æˆ–ä¸æŒ‡å®š
```

### å¤šGPUä¼˜åŒ–
```bash
# å……åˆ†åˆ©ç”¨GPUæ•°é‡
--num-clients 20  # 2ä¸ªGPU

# å¢å¤§batch size
--batch-size 256
```

## ğŸ” ç›‘æ§GPUä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹GPUå±æ€§
nvidia-smi --query-gpu=name,memory.total --format=csv
```

## ğŸ“ å®éªŒç¤ºä¾‹

### ç¤ºä¾‹1: MNISTåŸºå‡†æµ‹è¯•
```bash
python scripts/run_flower_example.py \
  --dataset mnist \
  --num-clients 10 \
  --rounds 10 \
  --batch-size 64
```

### ç¤ºä¾‹2: å¤šGPUåŠ é€Ÿè®­ç»ƒ
```bash
python scripts/run_multigpu_simple.py \
  --num-clients 40 \
  --rounds 20 \
  --batch-size 128
```

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- [Flower](https://flower.ai/) - è”é‚¦å­¦ä¹ æ¡†æ¶
- [SwanLab](https://swanlab.cn/) - å®éªŒè·Ÿè¸ª
- PyTorch - æ·±åº¦å­¦ä¹ æ¡†æ¶

---

**æç¤º**ï¼šæŸ¥çœ‹ [docs/](docs/) ç›®å½•è·å–è¯¦ç»†æ–‡æ¡£å’Œæ•™ç¨‹ã€‚
