#!/usr/bin/env python3
"""
ç»˜åˆ¶ Local Training vs Ours çš„ Benign Client Accuracy å¯¹æ¯”å›¾
æ”¯æŒä¸åŒçš„æŒ‡æ ‡é”®åï¼ˆlocalç”¨test_accuracy_mean, oursç”¨benign_test_accuracy_meanï¼‰
"""
from pathlib import Path
import json
import matplotlib.pyplot as plt
from matplotlib import rcParams

def load_history(path: Path):
    """åŠ è½½å†å²è®°å½•JSONæ–‡ä»¶"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_benign_accuracy(history, metric_key="benign_test_accuracy_mean"):
    """
    æå–è‰¯æ€§å®¢æˆ·ç«¯çš„æµ‹è¯•å‡†ç¡®ç‡

    ä¼˜å…ˆçº§:
    1. benign_test_accuracy_mean (oursæ–¹æ³•)
    2. test_accuracy_mean (localæ–¹æ³•)
    """
    rounds = []
    accuracies = []

    for entry in history:
        round_num = entry.get("round", len(rounds) + 1)
        aggregated = entry.get("aggregated", {})

        # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®å
        acc = (aggregated.get("benign_test_accuracy_mean") or
               aggregated.get("test_accuracy_mean") or
               None)

        if acc is not None:
            rounds.append(round_num)
            accuracies.append(acc)

    return rounds, accuracies

def plot_comparison(histories_dict, output_path, title="Benign Client Test Accuracy Comparison"):
    """ç»˜åˆ¶å¯¹æ¯”å›¾"""
    # è®¾ç½®å­—ä½“
    rcParams["font.family"] = "Times New Roman"

    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(10, 6))

    # é¢œè‰²å’Œæ ·å¼
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"]
    markers = ["o", "s", "D", "^"]
    linestyles = ["-", "--", "-.", ":"]

    # ç»˜åˆ¶æ¯ä¸ªæ–¹æ³•çš„æ›²çº¿
    for idx, (label, history_path) in enumerate(histories_dict.items()):
        history = load_history(Path(history_path))
        rounds, acc = extract_benign_accuracy(history)

        ax.plot(
            rounds,
            acc,
            label=label,
            color=colors[idx % len(colors)],
            marker=markers[idx % len(markers)],
            markersize=5,
            linewidth=2,
            linestyle=linestyles[idx % len(linestyles)],
            markevery=max(1, len(rounds) // 20),  # æ¯éš”ä¸€æ®µæ˜¾ç¤ºä¸€ä¸ªmarker
        )

    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    ax.set_xlabel("Training Round", fontsize=14, fontweight="bold")
    ax.set_ylabel("Benign Client Test Accuracy", fontsize=14, fontweight="bold")
    ax.set_title(title, fontsize=16, fontweight="bold", pad=20)

    # è®¾ç½®ç½‘æ ¼
    ax.grid(True, linestyle=":", linewidth=0.7, alpha=0.6)

    # è®¾ç½®å›¾ä¾‹
    ax.legend(
        loc="lower right",
        fontsize=12,
        frameon=True,
        shadow=True,
        fancybox=True,
    )

    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_xlim(0, max(rounds) + 5)
    ax.set_ylim(0, 1.0)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    # å®šä¹‰è¦å¯¹æ¯”çš„å®éªŒç»“æœ
    histories = {
        "Local Training (No FL)": "runs/local/20251106-212730-877045_local_100r_noise_random_retest/history.json",
        "Ours (Multi-Server FL + BALANCE)": "runs/ours/20251106-212741-350231_ours_100r_noise_random_retest/history.json",
    }

    # è¾“å‡ºè·¯å¾„
    output_path = "outputs/benign_acc_comparison.png"

    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plot_comparison(
        histories_dict=histories,
        output_path=output_path,
        title="Benign Client Test Accuracy: Local Training vs Multi-Server FL"
    )

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å®éªŒé…ç½®:")
    for label, path in histories.items():
        history = load_history(Path(path))
        rounds, acc = extract_benign_accuracy(history)
        print(f"\n{label}:")
        print(f"  - è®­ç»ƒè½®æ•°: {len(rounds)}")
        print(f"  - åˆå§‹å‡†ç¡®ç‡: {acc[0]:.4f}")
        print(f"  - æœ€ç»ˆå‡†ç¡®ç‡: {acc[-1]:.4f}")
        print(f"  - æœ€é«˜å‡†ç¡®ç‡: {max(acc):.4f} (Round {rounds[acc.index(max(acc))]})")
